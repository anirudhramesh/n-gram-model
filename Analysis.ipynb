{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the data we wrote into an excel file for the n-gram models and check if they're read correctly. Further, we import the actual json data provided as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneGram = pandas.read_csv('1-gram outputs.csv', index_col=0, header=0)\n",
    "twoGrams = pandas.read_csv('2-gram outputs.csv', index_col=0, header=0)\n",
    "origData = pandas.read_json('News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Frequency\n",
      "a                         134348\n",
      "aa                            14\n",
      "aaa                            7\n",
      "aaaaaaaaaaaaaaahhhhhh          1\n",
      "aaaaaah                        1\n",
      "          Frequency\n",
      "zaire             3\n",
      "zak               2\n",
      "zakar             1\n",
      "zakaria          12\n",
      "zakarian          2\n"
     ]
    }
   ],
   "source": [
    "print(oneGram.head())\n",
    "print(twoGrams.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the 1-gram list to see what the most common words are. Unfortunately, they are what are known as \"stop words\". In order to remove them, we use the natural language package to import a list of the most commonly used stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Frequency\n",
      "the     262419\n",
      "to      165065\n",
      "a       134348\n",
      "of      128470\n",
      "and     120303\n"
     ]
    }
   ],
   "source": [
    "oneGram.sort_values(by=['Frequency'], axis=0, ascending=False, inplace=True)\n",
    "print(oneGram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the stop words from the list and find that the new set of words is more sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Frequency\n",
      "new         18771\n",
      "trump       18252\n",
      "one         16352\n",
      "time        12288\n",
      "people      12267\n"
     ]
    }
   ],
   "source": [
    "oneGram.drop(stop_words, inplace=True, errors='ignore')\n",
    "print(oneGram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "rt_head = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Frequency\n",
      "of the      23037\n",
      "in the      20092\n",
      "it s        14745\n",
      "to the       9588\n",
      "to be        9160\n"
     ]
    }
   ],
   "source": [
    "twoGrams.sort_values(by=['Frequency'], ascending=False, inplace=True)\n",
    "print(twoGrams.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-3bdafd36d91f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-3bdafd36d91f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [wrd for stpwrd in stop_words for wrd not in ' ' + stpwrd]\u001b[0m\n\u001b[1;37m                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
