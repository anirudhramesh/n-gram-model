{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import numpy\n",
    "import main\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all the data we wrote into an excel file for the n-gram models and check if they're read correctly. Further, we import the actual json data provided as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneGram = pandas.read_csv('1-gram outputs.csv', index_col=0, header=0, keep_default_na=False, na_values=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoGrams = pandas.read_csv('2-gram outputs.csv', index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "origData = pandas.read_json('News_Category_Dataset_v2.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Frequency\n",
      "a                         134348\n",
      "aa                            14\n",
      "aaa                            7\n",
      "aaaaaaaaaaaaaaahhhhhh          1\n",
      "aaaaaah                        1\n",
      "Length: 84465\n",
      "\n",
      "\n",
      "               Frequency\n",
      "a a                   37\n",
      "a aaron                1\n",
      "a aawful               1\n",
      "a about                1\n",
      "a abstraction          1\n",
      "Length: 1797575\n"
     ]
    }
   ],
   "source": [
    "print(oneGram.head())\n",
    "print('Length: ' + str(len(oneGram)) + '\\n\\n')\n",
    "print(twoGrams.head())\n",
    "print('Length: ' + str(len(twoGrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort the 1-gram list to see what the most common words are. Unfortunately, they are what are known as \"stop words\". In order to remove them, we use the natural language toolkit to import a list of the most commonly used stop words. Further, we also want to remove plurals, and for that purpose we use the SnowballStemmer package inside nltk to find and remove all words with the same stem (except for one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Frequency\n",
      "aa                            14\n",
      "aaa                            7\n",
      "aaaaaaaaaaaaaaahhhhhh          1\n",
      "aaaaaah                        1\n",
      "aaaargh                        1\n"
     ]
    }
   ],
   "source": [
    "oneGram.drop(stop_words, inplace=True, errors='ignore')\n",
    "print(oneGram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Frequency\n",
      "new         18771\n",
      "trump       18252\n",
      "one         16352\n",
      "people      12267\n",
      "photos      11375\n"
     ]
    }
   ],
   "source": [
    "stemmedDict = dict(zip([stemmer.stem(ind) for ind in oneGram.index.tolist()], oneGram.index.tolist()))\n",
    "# creating a dict automatically drops duplicate values so the list above is unique\n",
    "oneGram.drop([ind for ind in oneGram.index.tolist() if ind not in stemmedDict[stemmer.stem(ind)]!=ind], inplace=True, errors='ignore')\n",
    "oneGram.sort_values(by=['Frequency'], axis=0, ascending=False, inplace=True)\n",
    "print(oneGram.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 2-gram list, we again remove any phrases containing stop-words since they do not convey any additional info and the remaining word that they are combined with would already be in the 1-gram list. This reduces the size considerably and allows for more complicated operations to be performed.\n",
    "\n",
    "The stemming part for 2-grams is a little more complicated, but essentially we separate the first and second word from the indexes into 2 separate lists. Then, we find stem words for the first word list, and piece together a unique list of 2 word strings by combining it back with the list of second words. Anything not appearing in this unique list is removed. We repeat the operation from the other side (find stem words for second word list and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Frequency\n",
      "of the      23037\n",
      "in the      20092\n",
      "it s        14745\n",
      "to the       9588\n",
      "to be        9160\n"
     ]
    }
   ],
   "source": [
    "twoGrams.sort_values(by=['Frequency'], ascending=False, inplace=True)\n",
    "print(twoGrams.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Frequency\n",
      "white house            1568\n",
      "health care            1249\n",
      "climate change         1107\n",
      "twitter facebook        888\n",
      "york city               691\n"
     ]
    }
   ],
   "source": [
    "# this cell takes a little bit of time, but the clean up ratio is worth it!\n",
    "\n",
    "deleter_column = []\n",
    "for key in twoGrams.index.tolist():\n",
    "    for stpword in stop_words:\n",
    "        if (stpword+' ' in key.split(' ')[0]+' ') or (' '+stpword in ' '+key.split(' ')[-1]):\n",
    "            deleter_column.append(True)\n",
    "            break\n",
    "        if stop_words.index(stpword) == len(stop_words)-1:\n",
    "            deleter_column.append(False)\n",
    "\n",
    "twoGrams['deleter_col']=deleter_column\n",
    "twoGrams.drop(twoGrams[twoGrams['deleter_col']==True].index, inplace=True)\n",
    "\n",
    "twoGrams.sort_values(by=['Frequency'], ascending=False, inplace=True)\n",
    "twoGrams.drop('deleter_col', axis=1, inplace=True)\n",
    "del deleter_column\n",
    "print(twoGrams.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstWordList = [ind.split(' ')[0] for ind in twoGrams.index.tolist()]\n",
    "secondWordList = [ind.split(' ')[1] for ind in twoGrams.index.tolist()]\n",
    "stemmedDict = dict(zip([stemmer.stem(firstWordList[ind]) + ' ' + secondWordList[ind] for ind in range(len(firstWordList))],\n",
    "                       twoGrams.index.tolist()))\n",
    "twoGrams.drop([ind for ind in twoGrams.index.tolist() if stemmedDict[stemmer.stem(ind.split(' ')[0])+' '+ind.split(' ')[1]]!=ind], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstWordList = [ind.split(' ')[0] for ind in twoGrams.index.tolist()]\n",
    "secondWordList = [ind.split(' ')[1] for ind in twoGrams.index.tolist()]\n",
    "stemmedDict = dict(zip([firstWordList[ind] + ' ' + stemmer.stem(secondWordList[ind]) for ind in range(len(firstWordList))],\n",
    "                       twoGrams.index.tolist()))\n",
    "twoGrams.drop([ind for ind in twoGrams.index.tolist() if stemmedDict[ind.split(' ')[0]+' '+stemmer.stem(ind.split(' ')[1])]!=ind], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We consider the word list to be sufficiently clean. As we can see, we have brought down the size of 1-grams to ~58,000 from the initial list of ~84,000. With 2-grams, we have done even better and brought down the list close to 200,000 from an initial list which was closer to 1.8 million. Now that we've cleaned the data set, we get down to analysis.\n",
    "\n",
    "To find the most important words, we try to negate the effect of \"filler words\" that have not been cleaned with the stop words list (such as \"new\", \"one\", \"time\" etc). To do this, we sort the 1-gram list by the product of word length and (log of) frequency, based on the intuition that filler words are shorter and we can reduce the impact of their frequency by taking log. This turns out to have commonality with the TF-IDF method, which isn't directly applicable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneGram['helper_col'] = oneGram.apply(lambda key: len(key.name)*math.log(key), axis=1)\n",
    "oneGram.sort_values(by=['helper_col'], axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same for the 2-gram list. Although we could use the 1-gram list to further enhance the 2-gram filter instead of repeating the process, we tend to lost information through that method. For example, while \"white\" may be a low importance filler word in the 1-gram analysis, \"white house\" would be an important phrase in the 2-gram list. Hence, using the importance of \"white\" from the 1-gram analysis does not add (and in fact, detracts) to our analysis of the 2-gram list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoGrams['helper_col'] = twoGrams.apply(lambda key: len(key.name)*math.log(key), axis=1)\n",
    "twoGrams.sort_values(by=['helper_col'], axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we take the original data into consideration. Since the dataset has a category column, we can use it to enhance our analysis, along with the headlines. We measure the importance of a single word by considering headlines within a given category, and measuring their occurrence within them. We make this into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDates = pandas.to_datetime(origData.date.unique())\n",
    "sourceDates = [dt for dt in sourceDates if dt.weekday()==4]\n",
    "\n",
    "categoryDataFrame = pandas.DataFrame(columns = origData.category.unique(), index = pandas.MultiIndex.from_product([[1,2], sourceDates]))\n",
    "del sourceDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-25 00:00:00\n",
      "2018-05-18 00:00:00\n",
      "2018-05-11 00:00:00\n",
      "2018-05-04 00:00:00\n",
      "2018-04-27 00:00:00\n",
      "2018-04-20 00:00:00\n",
      "2018-04-13 00:00:00\n",
      "2018-04-06 00:00:00\n",
      "2018-03-30 00:00:00\n",
      "2018-03-23 00:00:00\n",
      "2018-03-16 00:00:00\n",
      "2018-03-09 00:00:00\n",
      "2018-03-02 00:00:00\n",
      "2018-02-23 00:00:00\n",
      "2018-02-16 00:00:00\n",
      "2018-02-09 00:00:00\n",
      "2018-02-02 00:00:00\n",
      "2018-01-26 00:00:00\n",
      "2018-01-19 00:00:00\n",
      "2018-01-12 00:00:00\n",
      "2018-01-05 00:00:00\n",
      "2017-12-29 00:00:00\n",
      "2017-12-22 00:00:00\n",
      "2017-12-15 00:00:00\n",
      "2017-12-08 00:00:00\n",
      "2017-12-01 00:00:00\n",
      "2017-11-24 00:00:00\n",
      "2017-11-17 00:00:00\n",
      "2017-11-10 00:00:00\n",
      "2017-11-03 00:00:00\n",
      "2017-10-27 00:00:00\n",
      "2017-10-20 00:00:00\n",
      "2017-10-13 00:00:00\n",
      "2017-10-06 00:00:00\n",
      "2017-09-29 00:00:00\n",
      "2017-09-22 00:00:00\n",
      "2017-09-15 00:00:00\n",
      "2017-09-08 00:00:00\n",
      "2017-09-01 00:00:00\n",
      "2017-08-25 00:00:00\n",
      "2017-08-18 00:00:00\n",
      "2017-08-11 00:00:00\n",
      "2017-08-04 00:00:00\n",
      "2017-07-28 00:00:00\n",
      "2017-07-21 00:00:00\n",
      "2017-07-14 00:00:00\n",
      "2017-07-07 00:00:00\n",
      "2017-06-30 00:00:00\n",
      "2017-06-23 00:00:00\n",
      "2017-06-16 00:00:00\n",
      "2017-06-09 00:00:00\n",
      "2017-06-02 00:00:00\n",
      "2017-05-26 00:00:00\n",
      "2017-05-19 00:00:00\n",
      "2017-05-12 00:00:00\n",
      "2017-05-05 00:00:00\n",
      "2017-04-28 00:00:00\n",
      "2017-04-21 00:00:00\n",
      "2017-04-14 00:00:00\n",
      "2017-04-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "for dt in categoryDataFrame.index.get_level_values(1):\n",
    "    print(dt)\n",
    "    for cat in categoryDataFrame.columns:\n",
    "        weekData =  origData[origData.date.isin([dt-timedelta(days=i) for i in range(0,5)]) & origData.category.isin([cat])]\n",
    "        texts = ' '.join(weekData['headline'].tolist() + weekData['short_description'].tolist()).lower()\n",
    "            \n",
    "        # use our original n-grams module to create a new set of n-grams and compare them to the existing list\n",
    "        twoWord = main.ngram(texts, 2, writeToFile=False, returnFrame=True)\n",
    "        # make use of all the clean up done above to drop stop words, stem copies etc\n",
    "        twoWord = twoWord[twoWord.index.isin(twoGrams.index)]\n",
    "        twoWord = twoWord.assign(Frequency = twoGrams['helper_col'].loc[twoWord.index]).sort_values('Frequency', ascending=False)\n",
    "        categoryDataFrame[cat].loc['2', dt] = twoWord.T.to_dict(orient='list')\n",
    "        \n",
    "        oneWord = main.ngram(texts, 1, writeToFile=False, returnFrame=True)\n",
    "        oneWord = oneWord[oneWord.index.isin(oneGram.index)]\n",
    "        oneWord = oneWord.assign(Frequency = oneGram['helper_col'].loc[oneWord.index]).sort_values('Frequency', ascending=False)\n",
    "        categoryDataFrame[cat].loc['1', dt] = oneWord.T.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-205-1ef48760cafa>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-205-1ef48760cafa>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    oneWord.T.to_dict(orient='list'){:10}\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "oneWord.T.to_dict(orient='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-01-28 00:00:00')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origData.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>helper_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Frequency, helper_col]\n",
       "Index: []"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneGram[oneGram.Frequency==oneWord.Frequency.loc['students']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable            Type                    Data/Info\n",
      "-----------------------------------------------------\n",
      "SnowballStemmer     ABCMeta                 <class 'nltk.stem.snowball.SnowballStemmer'>\n",
      "cat                 str                     WELLNESS\n",
      "categoryDataFrame   DataFrame                            CRIME ENTERT<...>\\n[660 rows x 41 columns]\n",
      "datetime            module                  <module 'datetime' from '<...>37-32\\\\lib\\\\datetime.py'>\n",
      "deleter_column      list                    n=1797575\n",
      "dt                  Timestamp               2012-04-27 00:00:00\n",
      "firstWordList       list                    n=212390\n",
      "importlib           module                  <module 'importlib' from <...>\\importlib\\\\__init__.py'>\n",
      "key                 str                     zzzzzz gah\n",
      "main                module                  <module 'main' from 'C:\\\\<...>graham capital\\\\main.py'>\n",
      "math                module                  <module 'math' (built-in)>\n",
      "ngram               function                <function ngram at 0x17DD2D20>\n",
      "numpy               module                  <module 'numpy' from 'c:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "oneGram             DataFrame                                        <...>n[69183 rows x 2 columns]\n",
      "oneWord             DataFrame               Empty DataFrame\\nColumns: [Frequency]\\nIndex: []\n",
      "origData            DataFrame                                        <...>[200853 rows x 6 columns]\n",
      "pandas              module                  <module 'pandas' from 'c:<...>es\\\\pandas\\\\__init__.py'>\n",
      "secondWordList      list                    n=212390\n",
      "stemmedDict         dict                    n=201103\n",
      "stemmer             SnowballStemmer         <nltk.stem.snowball.Snowb<...>mer object at 0x1262DF90>\n",
      "stop_words          list                    n=179\n",
      "stopwords           WordListCorpusReader    <WordListCorpusReader in <...>ata\\\\corpora\\\\stopwords'>\n",
      "stpword             str                     wouldn't\n",
      "texts               str                     ira israel: happiness wei<...>\" has officially arrived.\n",
      "timedelta           type                    <class 'datetime.timedelta'>\n",
      "twoGrams            DataFrame                                        <...>[201103 rows x 2 columns]\n",
      "twoWord             DataFrame               Empty DataFrame\\nColumns: [Frequency]\\nIndex: []\n",
      "weekData            DataFrame                                        <...>n\\n[119 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "whos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
